{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermal Conductivity Fit Analysis\n",
    "Developed by Henry Nachman\n",
    "\n",
    "Last Edited: 22 February 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# note : most functions needed for running this notebook can be found in tc_utils.\n",
    "from tc_utils import *\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find where all our RAW data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aluminum': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\Aluminum\\\\RAW', 'CFRP': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\CFRP\\\\RAW', 'Clearwater': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\Clearwater\\\\RAW', 'DPP': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\DPP\\\\RAW', 'Fiberglass': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\Fiberglass\\\\RAW', 'Graphlite': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\Graphlite\\\\RAW', 'Ketron': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\Ketron\\\\RAW', 'Macor': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\Macor\\\\RAW', 'POCO Graphite': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\POCO Graphite\\\\RAW', 'SS304': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\SS304\\\\RAW', 'Torlon': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\Torlon\\\\RAW', 'VESPEL': 'c:\\\\Users\\\\henac\\\\OneDrive - The University of Texas at Austin\\\\01_RESEARCH\\\\05_CMBS4\\\\Cryogenic_Material_Properties\\\\thermal_conductivity\\\\lib\\\\VESPEL\\\\RAW'}\n"
     ]
    }
   ],
   "source": [
    "path_to_lib = f\"{os.getcwd()}\\\\lib\"\n",
    "mat_directories = [folder for folder in os.listdir(path_to_lib) if not folder.endswith(\".md\")]\n",
    "\n",
    "path_to_RAW = dict()\n",
    "for mat in mat_directories:\n",
    "    raw_str = f\"{path_to_lib}\\\\{mat}\\\\RAW\"\n",
    "    path_to_RAW[mat] = raw_str\n",
    "\n",
    "\n",
    "print(path_to_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will fit the data to polynomials. The fit is done in two stages, a low end fit and a high end fit. The two fits are then connected using an Error Function centered at the discontinuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_tc_fit(big_data, save_path, erf_loc = 20, fit_orders = (3,3), fit_types=(\"k/T\", \"loglog\"), plots=False):\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - big_data   - Array of measurement data concatenated (should be of shape: [N, 3])\n",
    "    - save_path  - File path to publish output files and plots.\n",
    "    - erf_loc     - default=20    - Temperature at which to split the data for fitting (and to place the error function).\n",
    "    - fit_orders - default=(3,3) - Polynomial fit order (low, high).\n",
    "    - fit_types  - default=(\"k/T\", \"loglog\") - defines the type of fit for each regime (low, high).\n",
    "    - plots      - default=False - Boolean argument, if true, plots are made and saved to save_path.\n",
    "\n",
    "    Returns :\n",
    "    - arg_dict - Dictionary of fit arguments - includes low fit, high fit, and combined fit arguments.\n",
    "    \"\"\"\n",
    "    \n",
    "    dsplit = split_data(big_data, erf_loc)\n",
    "    lowT, lowT_k, lowT_koT, low_ws, hiT, hiT_k, hiT_koT, hi_ws = dsplit\n",
    "\n",
    "    # Take a log10 of the high range\n",
    "    log_hi_T = np.log10(hiT)\n",
    "    log_hi_k = np.log10(hiT_k)\n",
    "    # Fit the low data\n",
    "    try:\n",
    "        if (fit_types[0] == \"k/T\") and (len(lowT)!=0):\n",
    "            low_fit_xs, low_fit = koT_function(lowT, lowT_koT, fit_orders[0], low_ws)\n",
    "        elif (len(lowT)==0):\n",
    "            low_fit = [0]\n",
    "            print(f\"Only using high fit {min(T)} > 20\")\n",
    "\n",
    "        # Fit the high data\n",
    "        \n",
    "        if fit_types[1] == \"loglog\" and (len(hiT)!=0):\n",
    "            hi_fit_xs, hi_fit = logk_function(log_hi_T, log_hi_k, fit_orders[1], hi_ws)\n",
    "        elif (len(hiT)==0):\n",
    "            hi_fit = [0]\n",
    "            print(f\"Only using low fit {max(T)} < 20\")\n",
    "\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"LinAlgError - likely not enough points after weight to fit the data.\")\n",
    "        raise np.linalg.LinAlgError()\n",
    "        \n",
    "    # # Combine the fits\n",
    "    # xrange_total = np.linspace(min(lowT), max(hiT), 100)\n",
    "    # logk = loglog(xrange_total, low_poly1d, hi_poly1d, erf_place)\n",
    "    # #\n",
    "\n",
    "    if plots:\n",
    "        fig, axs = plt.subplots(2, figsize=(8, 6))\n",
    "        axs[0].plot(lowT, lowT_koT,'.')\n",
    "        axs[0].plot(low_fit_xs, np.polyval(low_fit, low_fit_xs))\n",
    "        axs[0].set_xlabel(\"T\")\n",
    "        axs[0].set_ylabel(\"k/T\")\n",
    "        axs[0].title.set_text(\"Low Temperature Fit\")\n",
    "        axs[1].loglog(10**hi_fit_xs, 10**np.polyval(hi_fit, hi_fit_xs))\n",
    "        axs[1].loglog(hiT, hiT_k, '.')\n",
    "        axs[1].grid(True, which=\"both\", ls=\"-\", color='0.65')\n",
    "        axs[1].set_ylabel(\"k\")\n",
    "        axs[1].set_xlabel(\"T\")\n",
    "        axs[1].title.set_text(\"High Temperature Fit\")\n",
    "        plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "        plt.savefig(f\"{save_path}\\\\fits_subplots.pdf\", dpi = 300, format=\"pdf\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "    arg_dict = make_arg_dict(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "    return arg_dict\n",
    "\n",
    "def koT_function(T, koT, orders, weights):\n",
    "    low_fit_xs = np.linspace(np.min(T), np.max(T), 100)\n",
    "    lofit_full = np.polyfit(T, koT, orders, full=True, w=weights)\n",
    "    low_fit, residuals_lo, rank_lo, sing_vals_lo, rcond_lo = lofit_full\n",
    "    low_poly1d = np.poly1d(low_fit)\n",
    "    return low_fit_xs, low_fit\n",
    "\n",
    "def logk_function(logT, logk, orders, weights):\n",
    "    fit_T = np.linspace(np.min(logT), np.max(logT), 100)\n",
    "    fit_full = np.polyfit(logT, logk, orders, full=True, w=weights)\n",
    "    fit, residuals_hi, rank_hi, sing_vals_hi, rcond_hi =  fit_full\n",
    "    # hi_poly1d = np.poly1d(fit)\n",
    "    return fit_T, fit\n",
    "\n",
    "def make_arg_dict(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc):\n",
    "    low_func = f\"{fit_orders[0]} order {fit_types[0]}\"\n",
    "    hi_func = f\"{fit_orders[1]} order {fit_types[1]}\"\n",
    "    \n",
    "    low_param = np.array(low_fit)\n",
    "    hi_param = np.array(hi_fit)\n",
    "    \n",
    "    all_params = np.append(np.append(low_param, hi_param), erf_loc)\n",
    "\n",
    "    arg_dict = {\"low_function_type\"  : low_func,\n",
    "                \"low_fit_param\"      : low_param.tolist(),\n",
    "                \"low_fit_range\"      : np.array([min(low_fit_xs), max(low_fit_xs)]).tolist(),\n",
    "                \"hi_function_type\"   : hi_func,\n",
    "                \"hi_fit_param\"       : hi_param.tolist(),\n",
    "                \"hi_fit_range\"       : np.array([10**min(hi_fit_xs), 10**max(hi_fit_xs)]).tolist(),\n",
    "                \"combined_function_type\" : \"loglog\",\n",
    "                \"combined_fit_param\" : all_params.tolist(),\n",
    "                \"combined_fit_range\" : np.array([min(min(low_fit_xs), 10**min(hi_fit_xs)), max(max(low_fit_xs), 10**max(hi_fit_xs))]).tolist()}\n",
    "    return arg_dict\n",
    "\n",
    "def split_data(big_data, erf_loc):\n",
    "    # divide the data array into three columns\n",
    "    T, k, koT, weights = [big_data[:,0], big_data[:,1], big_data[:,2], big_data[:,3]]\n",
    "\n",
    "    \n",
    "\n",
    "    # Find the low range\n",
    "    # lowT, hiT = [T[T<erf_loc], T[T>erf_loc]]\n",
    "    \n",
    "    # if (len(lowT) == 0) or (len(hiT) ==0):\n",
    "    #     print(\"ERROR  - data split results in 0-length array, please adjust split location\")\n",
    "    #     print(f\"NOTE   - min(T) = {min(T)}, max(T) = {max(T)} \")\n",
    "    #     print(f\"{(len(lowT))} {(len(hiT))}\")\n",
    "\n",
    "    #     erf_loc = np.mean(T)\n",
    "\n",
    "    low_ws, hi_ws = [weights[T<erf_loc], weights[T>erf_loc]]\n",
    "\n",
    "    # Find the low range\n",
    "    lowT, lowT_k, lowT_koT = [T[T<erf_loc], k[T<erf_loc], koT[T<erf_loc]]\n",
    "    \n",
    "    # Find the high range\n",
    "    hiT, hiT_k, hiT_koT = [T[T>erf_loc], k[T>erf_loc], koT[T>erf_loc]]\n",
    "\n",
    "    return [lowT, lowT_k, lowT_koT, low_ws, hiT, hiT_k, hiT_koT, hi_ws]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log10 k = Log10 [aT+bT^2+cT^3+3E-9T^5]*0.5*[1-ERF(15(Log10T-1.48)]+[d+eLog10T+f(Log10T)^2+g(log10T)^3+hEXP^(Log10T)]*0.5*[1+ERF(15(Log10T-1.48)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_table(data, output_file):\n",
    "    \"\"\"\n",
    "    Description : Formats a dictionary in string style for saving to text file formats and saves to txt file.\n",
    "    \"\"\"\n",
    "    # Extract column names from the first dictionary\n",
    "    longest = 0\n",
    "    longest_arg = 0\n",
    "    for i in range(len(data)):\n",
    "        if len(data[i].keys())>longest:\n",
    "            longest = len(data[i].keys())\n",
    "            longest_arg = i\n",
    "    \n",
    "    columns = list(data[longest_arg].keys())\n",
    "    # Find the maximum width for each column\n",
    "    column_widths = {column: (len(str(column))+12) for column in columns}\n",
    "    # Open the output file in write mode\n",
    "    with open(output_file, 'w') as file:\n",
    "        # Write the header row\n",
    "        file.write('| ' + ' | '.join(column.ljust(column_widths[column]) for column in columns) + ' |\\n')\n",
    "        # Write the separator row\n",
    "        file.write('| ' + '---'.join(['-' * column_widths[column] for column in columns]) + ' |\\n')\n",
    "\n",
    "        for row in data:\n",
    "            file.write(\"| \")\n",
    "            write_row = []\n",
    "            for param in columns:\n",
    "                if np.isin(param, list(row.keys())):\n",
    "                    write_row.append(str(row[param]))\n",
    "                else:\n",
    "                    write_row.append(\"^\")\n",
    "            for i in range(len(write_row)):\n",
    "                file.write(''.join(write_row[i]).ljust(column_widths[list(column_widths.keys())[i]])+\" | \")\n",
    "            file.write('\\n')\n",
    "    return\n",
    "\n",
    "def create_tc_csv(data, output_file):\n",
    "    \"\"\"\n",
    "    Description : Formats a dictionary and saves to csv file.\n",
    "    \"\"\"\n",
    "    # Extract column names from the first dictionary\n",
    "    longest = 0\n",
    "    longest_arg = 0\n",
    "    for i in range(len(data)):\n",
    "        if len(data[i].keys())>longest:\n",
    "            longest = len(data[i].keys())\n",
    "            longest_arg = i\n",
    "    # Extract column names from the first dictionary\n",
    "    columns = list(data[longest_arg].keys())\n",
    "        # Open the output file in write mode with newline='' to ensure consistent line endings\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        # Create a CSV writer object\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write the header row\n",
    "        csv_writer.writerow(columns)\n",
    "\n",
    "        # Write each data row\n",
    "        for row in data:\n",
    "            write_row = []\n",
    "            for param in columns:\n",
    "                if np.isin(param, list(row.keys())):\n",
    "                    write_row.append(str(row[param]))\n",
    "                else:\n",
    "                    write_row.append(\"-\")\n",
    "            csv_writer.writerow(write_row)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our fitting code for every material found in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a combined fit - data exists on both sides of 20K.\n",
      "[3.8460000e-01 1.1978570e+02 2.3918680e+02 3.5858790e+02 4.7798900e+02\n",
      " 5.9739010e+02 7.1679120e+02 8.3619230e+02 9.5559340e+02 1.0749945e+03\n",
      " 1.1943956e+03 1.3137967e+03 1.4331978e+03 1.5525989e+03 1.6720000e+03]\n",
      "Only using high fit 0.3846 > 20\n",
      "Only using low fit 1672.0 < 20\n",
      "Low-Hi split centered at : 119.7857 ~~ with average percent difference value of: 3.95%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tc_utils import *\n",
    "\n",
    "for mat in path_to_RAW.keys(): # [\"SS304\"]: # \n",
    "    perc_diff_avg = np.array([])\n",
    "    ## First, let's collect the raw data from their csv files\n",
    "    big_data, data_dict = parse_raw(mat, path_to_RAW[mat], plots=False, weight_const=0.00)\n",
    "    T, k, koT, weights = [big_data[:,0], big_data[:,1], big_data[:,2], big_data[:,3]]\n",
    "\n",
    "    maxT, minT = [max(T), min(T)]\n",
    "    fit_orders = [3,3]\n",
    "    fit_types = [\"k/T\", \"loglog\"]\n",
    "\n",
    "    if (maxT <= 20):\n",
    "        print(f\"Using a low fit - {maxT} is below 20K.\")\n",
    "        low_fit_xs, low_fit = koT_function(T, koT, fit_orders[0], weights)\n",
    "        hi_fit, hi_fit_xs, erf_loc = [[0], [0], [0]]\n",
    "        fit_args = make_arg_dict(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "    elif (minT >= 20):\n",
    "        print(f\"Using a hi fit - {minT} is above 20K.\")\n",
    "        hi_fit_xs, hi_fit = logk_function(np.log10(T), np.log10(k), fit_orders[1], weights)\n",
    "        low_fit, low_fit_xs, erf_loc = [[0], [0], [-1]]\n",
    "        fit_args = make_arg_dict(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "    else:\n",
    "        print(f\"Using a combined fit - data exists on both sides of 20K.\")\n",
    "        erf_locList = np.linspace(np.sort(T)[0], np.sort(T)[-1], 15) # [30] # \n",
    "        print(erf_locList)\n",
    "        for erf_loc in erf_locList:\n",
    "            dsplit = split_data(big_data, erf_loc)\n",
    "            lowT, lowT_k, lowT_koT, low_ws, hiT, hiT_k, hiT_koT, hi_ws = dsplit\n",
    "            # Take a log10 of the high range\n",
    "            log_hi_T = np.log10(hiT)\n",
    "            log_hi_k = np.log10(hiT_k)\n",
    "            \n",
    "            if (len(lowT)==0):\n",
    "                low_fit = [0]\n",
    "                print(f\"Only using high fit {min(T)} > 20\")\n",
    "            else:\n",
    "                low_fit_xs, low_fit = koT_function(lowT, lowT_koT, fit_orders[0], low_ws)\n",
    "            if (len(hiT)==0):\n",
    "                hi_fit = [0]\n",
    "                print(f\"Only using low fit {max(T)} < 20\")\n",
    "            else:\n",
    "                hi_fit_xs, hi_fit = logk_function(log_hi_T, log_hi_k, fit_orders[1], hi_ws)\n",
    "            fit_args = make_arg_dict(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "            ## With the fit complete, let's output a formatted dictionary with the fit parameters\n",
    "            output_array = make_fit_dict(fit_args)\n",
    "            ## We want to figure out the best location for the split in data, so we will compute the residual of the combined fit\n",
    "            # Tdata = np.concatenate([(data_dict[ref_name].T[0]) for ref_name in data_dict])\n",
    "            # kdata = np.concatenate([(data_dict[ref_name].T[1]) for ref_name in data_dict])\n",
    "            low_param, hi_param, erf_param = fit_args[\"low_fit_param\"], fit_args[\"hi_fit_param\"], fit_args[\"combined_fit_param\"][-1]\n",
    "            kpred = loglog_func(T, low_param, hi_param, erf_param)\n",
    "            # and append it to the array resVal\n",
    "            diff = abs(kpred-k)\n",
    "            perc_diff_arr = 100*diff/kpred\n",
    "            # print((kpred-kpred*perc_diff_arr)[Tdata.argsort()])\n",
    "            perc_diff_avg = np.append(perc_diff_avg, np.mean(perc_diff_arr))\n",
    "\n",
    "        # plt.plot(erf_locList, perc_diff_avg)\n",
    "        # plt.show()\n",
    "        # Now that we have found the residuals of the fits for many different split locations, let's choose the best one.    \n",
    "        erf_locdict = dict(zip(erf_locList, perc_diff_avg))\n",
    "        bestRes = min(erf_locdict.values())\n",
    "        besterf_loc = [key for key in erf_locdict if erf_locdict[key] == bestRes]\n",
    "        print(f\"Low-Hi split centered at : {besterf_loc[0]} ~~ with average percent difference value of: {bestRes:.2f}%\")\n",
    "        \n",
    "        # We will repeat the above fit with this new 'optimized' split location\n",
    "        fit_args = dual_tc_fit(big_data, os.path.split(path_to_RAW[mat])[0], erf_loc=besterf_loc, fit_orders=(3, 3), plots=False)\n",
    "    \n",
    "    output_array = make_fit_dict(fit_args)\n",
    "\n",
    "    # Finally, we will output the fit parameters as a csv, and lh5 file - and plot the data.\n",
    "    create_data_table(output_array, f\"{os.path.split(path_to_RAW[mat])[0]}\\\\{os.path.split(os.path.split(path_to_RAW[mat])[0])[1]}.txt\")\n",
    "    create_tc_csv(output_array, f\"{os.path.split(path_to_RAW[mat])[0]}\\\\{os.path.split(os.path.split(path_to_RAW[mat])[0])[1]}.csv\")\n",
    "    make_fit_lh5(fit_args, os.path.split(path_to_RAW[mat])[0])\n",
    "    # PLOTTING CODE\n",
    "    tk_plot(mat,path_to_RAW, data_dict, fit_args, fit_range = [100e-3, np.sort(T)[-1]], points=True, fits=\"combined\", fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets output a single human readable (and CSV) file of the fits for each material currently in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array = compile_csv(path_to_RAW)\n",
    "create_data_table(output_array, \"..\\\\thermal_conductivity_compilation.txt\")\n",
    "create_tc_csv(output_array, \"..\\\\thermal_conductivity_compilation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENAPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
