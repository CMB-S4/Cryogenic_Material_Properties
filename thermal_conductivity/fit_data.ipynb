{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermal Conductivity Raw Data Fitting\n",
    "Developed by Henry Nachman\n",
    "\n",
    "Last Edited: 26 March 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json, shutil\n",
    "\n",
    "# note : most functions needed for running this notebook can be found in tc_utils.\n",
    "from tc_utils import *\n",
    "# Defines the matplotlib backend for plots\n",
    "# %matplotlib qt5\n",
    "\n",
    "plots = True # Set to true to reproduce all plots, note this will likely lengthen the time to run the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find where all our RAW data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_lib = f\"{os.getcwd()}\\\\lib\"\n",
    "mat_directories = [folder for folder in os.listdir(path_to_lib) if not folder.endswith(\".md\")]\n",
    "\n",
    "path_to_RAW = dict()\n",
    "\n",
    "for mat in mat_directories:\n",
    "    path_to_mat = f\"{path_to_lib}\\\\{mat}\"\n",
    "    raw_str = f\"{path_to_mat}\\\\RAW\"\n",
    "    config_str = f\"{path_to_mat}\\\\config.yaml\"\n",
    "    parent_yaml_str = f\"{path_to_mat}\\\\parent.yaml\"\n",
    "    other_str = f\"{path_to_mat}\\\\OTHERFITS\"\n",
    "    nist_str = f\"{path_to_mat}\\\\NIST\"\n",
    "    source = []\n",
    "    if os.path.exists(raw_str): # Finds the raw data if it exists.\n",
    "        path_to_RAW[mat] = raw_str\n",
    "        source.append(\"RAW\")\n",
    "    if os.path.exists(other_str): # Finds other fits\n",
    "        source.append(\"other\")\n",
    "    if os.path.exists(nist_str): # Finds NIST fit\n",
    "        source.append(\"NIST\")\n",
    "\n",
    "    if not os.path.exists(config_str): # Check for existing JSON\n",
    "        parent = \"NA\"\n",
    "    else:\n",
    "        with open(config_str, 'r') as file:\n",
    "            mat_config = json.load(file)\n",
    "        parent = mat_config[0][\"parent\"]\n",
    "\n",
    "    yaml_dict = []\n",
    "    for i in range(len(source)):\n",
    "        yaml_dict.append({\"name\":f\"{mat}\", \"parent\":f\"{parent}\", \"source\":f\"{source[i]}\"}) # Define JSON dictionary\n",
    "    yaml_dict = json.dumps(yaml_dict, indent=4)\n",
    "    with open(config_str, 'w') as file:\n",
    "        file.write(yaml_dict) # Write to new JSON\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRP_Clearwater has parent: CFRP\n",
      "Found 5 measurements.\n",
      "CFRP_DPP has parent: CFRP\n",
      "Found 2 measurements.\n",
      "CFRP_Graphlite has parent: CFRP\n",
      "Found 2 measurements.\n",
      "Stainless_Steel_304 has parent: Stainless_Steel\n",
      "Found 8 measurements.\n",
      "Stainless_Steel_310 has parent: Stainless_Steel\n",
      "Found 3 measurements.\n",
      "Stainless_Steel_316 has parent: Stainless_Steel\n",
      "Found 6 measurements.\n",
      "Stainless_Steel_321 has parent: Stainless_Steel\n",
      "Found 6 measurements.\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON\n",
    "for mat in mat_directories:\n",
    "    path_to_mat = f\"{path_to_lib}\\\\{mat}\"\n",
    "    raw_str = f\"{path_to_mat}\\\\RAW\"\n",
    "    config_str = f\"{path_to_mat}\\\\config.yaml\"\n",
    "\n",
    "    if os.path.exists(raw_str):\n",
    "        # other_str = f\"{path_to_mat}\\\\OTHERFITS\"\n",
    "        with open(config_str, 'r') as file:\n",
    "            mat_config = json.load(file)\n",
    "        parent = mat_config[0][\"parent\"]\n",
    "        if parent != \"NA\":\n",
    "            print(mat, \"has parent:\", parent)\n",
    "            parent_dir = f\"{path_to_lib}\\\\{parent}\"\n",
    "            if not os.path.exists(parent_dir):\n",
    "                os.mkdir(parent_dir)\n",
    "                os.mkdir(f\"{parent_dir}\\\\RAW\")\n",
    "            raw_files = get_datafiles(raw_str)\n",
    "            for file in raw_files:\n",
    "                # print(file)\n",
    "                # try:\n",
    "                shutil.copy(f\"{raw_str}\\\\{file}\", f\"{parent_dir}\\\\RAW\\\\{file}\")\n",
    "                # except shutil.SameFileError:\n",
    "                    # print(\"file already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our fitting code for every material found in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_RAW = dict()\n",
    "path_to_fits = dict()\n",
    "path_to_plots = dict()\n",
    "\n",
    "for mat in mat_directories:\n",
    "    path_to_mat = f\"{path_to_lib}\\\\{mat}\"\n",
    "    raw_str = f\"{path_to_mat}\\\\RAW\"\n",
    "    fits_str = f\"{path_to_mat}\\\\fits\"\n",
    "    plots_str = f\"{path_to_mat}\\\\plots\"\n",
    "    if os.path.exists(raw_str):\n",
    "        path_to_RAW[mat] = raw_str\n",
    "        if not os.path.exists(fits_str):\n",
    "            os.mkdir(fits_str)\n",
    "        path_to_fits[mat] = fits_str\n",
    "        if not os.path.exists(plots_str):\n",
    "            os.mkdir(plots_str)\n",
    "        path_to_fits[mat] = fits_str\n",
    "        path_to_plots[mat] = plots_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gaps(data_array, threshold):\n",
    "    # Calculate differences between consecutive elements\n",
    "    diffs = np.diff(np.sort(data_array))\n",
    "    # Find indices where differences exceed threshold\n",
    "    major_gap_indices = np.where(diffs > threshold)[0]\n",
    "\n",
    "    # Return indices of major gaps\n",
    "    return major_gap_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aluminum_110 : Using a low fit\n",
      "CFRP : Using a low fit\n",
      "CFRP_Clearwater : Using a low fit\n",
      "CFRP_DPP : Using a low fit\n",
      "CFRP_Graphlite : Using a low fit\n",
      "G10_FR4 : Using a low fit\n",
      "Ketron : Using a low fit\n",
      "Macor : Using a low fit\n",
      "Stainless_Steel : Using a combined fit\n",
      "Low-Hi split centered at : 119.47072857142858 ~~ with average percent difference value of: 6.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henac\\OneDrive - The University of Texas at Austin\\01_RESEARCH\\05_CMBS4\\Cryogenic_Material_Properties\\thermal_conductivity\\fit_types.py:29: RuntimeWarning: overflow encountered in power\n",
      "  hi_fit = 10**np.polyval(hi_param, np.log10(T))\n",
      "c:\\Users\\henac\\OneDrive - The University of Texas at Austin\\01_RESEARCH\\05_CMBS4\\Cryogenic_Material_Properties\\thermal_conductivity\\fit_types.py:41: RuntimeWarning: invalid value encountered in multiply\n",
      "  k = low_fit*erf_low+hi_fit*erf_hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stainless_Steel_304 : Using a combined fit\n",
      "Low-Hi split centered at : 119.7857 ~~ with average percent difference value of: 3.95%\n"
     ]
    }
   ],
   "source": [
    "for mat in  path_to_RAW.keys(): # [\"CFRP\"]: #\n",
    "    perc_diff_avgs = np.array([])\n",
    "    ## First, let's collect the raw data from their csv files\n",
    "    big_data, data_dict = parse_raw(mat, path_to_RAW[mat], plots=False, weight_const=0.00)\n",
    "    T, k, koT, weights = [big_data[:,0], big_data[:,1], big_data[:,2], big_data[:,3]]\n",
    "\n",
    "    gaps = len(find_gaps(T, 100)) > 0\n",
    "    # print(find_gaps(T, 100))\n",
    "    maxT, minT = [max(T), min(T)]\n",
    "    fit_orders = [3,3]\n",
    "    fit_types = [\"k/T\", \"loglog\"]\n",
    "\n",
    "    lenLow = len(T[T<=20])\n",
    "    lenHi = len(T[T>=20])\n",
    "\n",
    "    if lenHi==0:\n",
    "        print(f\"{mat} : Using a low fit\")\n",
    "        low_fit_xs, low_fit = koT_function(T, koT, fit_orders[0], weights)\n",
    "        hi_fit, hi_fit_xs, erf_loc = [[0], [0], [0]]\n",
    "        fit_args = dict_combofit(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "        perc_diff_low, perc_diff_arr = get_percdiff(T[T<=max(low_fit_xs)],k[T<=max(low_fit_xs)], fit_args)\n",
    "        fit_args[\"low_perc_err\"] =  perc_diff_low\n",
    "        fit_args[\"hi_perc_err\"] =  0\n",
    "        fit_args[\"combined_perc_err\"] =  perc_diff_low\n",
    "    elif lenLow==0:\n",
    "        print(f\"{mat} : Using a hi fit\")\n",
    "        hi_fit_xs, hi_fit = logk_function(np.log10(T), np.log10(k), fit_orders[1], weights)\n",
    "        low_fit, low_fit_xs, erf_loc = [[0], [0], [-1]]\n",
    "        fit_args = dict_combofit(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "        perc_diff_hi, perc_diff_arr = get_percdiff(T[T>=min(hi_fit_xs)],k[T>=min(hi_fit_xs)], fit_args)\n",
    "        fit_args[\"hi_perc_err\"] = perc_diff_hi\n",
    "        fit_args[\"low_perc_err\"] =  0\n",
    "        fit_args[\"combined_perc_err\"] =  perc_diff_hi\n",
    "\n",
    "    else:\n",
    "        print(f\"{mat} : Using a combined fit\")# - data exists on both sides of 20K\")\n",
    "        erf_locList = np.linspace(np.sort(T)[0], np.sort(T)[-1], 15)\n",
    "        for erf_loc in erf_locList:\n",
    "            dsplit = split_data(big_data, erf_loc)\n",
    "            lowT, lowT_k, lowT_koT, low_ws, hiT, hiT_k, hiT_koT, hi_ws = dsplit\n",
    "            # Take a log10 of the high range\n",
    "            log_hi_T = np.log10(hiT)\n",
    "            log_hi_k = np.log10(hiT_k)\n",
    "            \n",
    "            if (len(lowT)==0):\n",
    "                low_fit = [0]\n",
    "                low_fit_xs = [0]\n",
    "            else:\n",
    "                low_fit_xs, low_fit = koT_function(lowT, lowT_koT, fit_orders[0], low_ws)\n",
    "            if (len(hiT)==0):\n",
    "                hi_fit = [0]\n",
    "                hi_fit_xs = [0]\n",
    "            else:\n",
    "                hi_fit_xs, hi_fit = logk_function(log_hi_T, log_hi_k, fit_orders[1], hi_ws)\n",
    "            fit_args = dict_combofit(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "            ## With the fit complete, let's output a formatted dictionary with the fit parameters\n",
    "            # output_array = format_combofit(fit_args)\n",
    "            ## We want to figure out the best location for the split in data, so we will compute the residual of the combined fit\n",
    "            low_param, hi_param, erf_param = fit_args[\"low_fit_param\"], fit_args[\"hi_fit_param\"], fit_args[\"combined_fit_param\"][-1]\n",
    "            # kpred = loglog_func(T, low_param, hi_param, erf_param)\n",
    "            # and append it to the array resVal\n",
    "            # diff = kpred-k\n",
    "            # perc_diff_arr = 100*abs(diff/kpred)\n",
    "            perc_diff_avg, perc_diff_arr = get_percdiff(T, k, fit_args)\n",
    "            perc_diff_avgs = np.append(perc_diff_avgs, perc_diff_avg)\n",
    "\n",
    "        # Now that we have found the residuals of the fits for many different split locations, let's choose the best one.    \n",
    "        erf_locdict = dict(zip(erf_locList, perc_diff_avgs))\n",
    "        bestRes = min(erf_locdict.values())\n",
    "        besterf_loc = [key for key in erf_locdict if erf_locdict[key] == bestRes]\n",
    "        \n",
    "        # We will repeat the above fit with this new 'optimized' split location\n",
    "        fit_args = dual_tc_fit(big_data, path_to_plots[mat], erf_loc=min(besterf_loc), fit_orders=fit_orders, plots=False)\n",
    "        perc_diff_avg, perc_diff_arr = get_percdiff(T, k, fit_args)\n",
    "        print(f\"Low-Hi split centered at : {min(besterf_loc)} ~~ with average percent difference value of: {perc_diff_avg:.2f}%\")\n",
    "\n",
    "        perc_diff_low, perc_diff_arr = get_percdiff(T[T<=max(low_fit_xs)],k[T<=max(low_fit_xs)], fit_args)\n",
    "        perc_diff_hi, perc_diff_arr = get_percdiff(T[T>=min(hi_fit_xs)],k[T>=min(hi_fit_xs)], fit_args)\n",
    "        perc_diff_combo, perc_diff_arr = get_percdiff(T,k, fit_args)\n",
    "        fit_args[\"low_perc_err\"] =  perc_diff_low\n",
    "        fit_args[\"hi_perc_err\"] =  perc_diff_hi\n",
    "        fit_args[\"combined_perc_err\"] =  perc_diff_combo\n",
    "\n",
    "    if gaps:\n",
    "        low_array = format_splitfit(fit_args, \"low\")\n",
    "        high_array = format_splitfit(fit_args, \"hi\")\n",
    "\n",
    "        create_data_table(low_array, f\"{path_to_fits[mat]}\\\\{mat}_lo.txt\")\n",
    "        create_tc_csv(low_array, f\"{path_to_fits[mat]}\\\\{mat}_lo.csv\")\n",
    "        create_data_table(high_array, f\"{path_to_fits[mat]}\\\\{mat}_hi.txt\")\n",
    "        create_tc_csv(high_array, f\"{path_to_fits[mat]}\\\\{mat}_hi.csv\")\n",
    "        make_fit_lh5(fit_args, path_to_fits[mat])\n",
    "\n",
    "    else:\n",
    "        output_array = format_combofit(fit_args)\n",
    "        # Finally, we will output the fit parameters as a csv, and lh5 file - and plot the data.\n",
    "        create_data_table(output_array, f\"{path_to_fits[mat]}\\\\{mat}.txt\")\n",
    "        create_tc_csv(output_array, f\"{path_to_fits[mat]}\\\\{mat}.csv\")\n",
    "        make_fit_lh5(fit_args, path_to_fits[mat])\n",
    "        # PLOTTING CODE\n",
    "    if plots:\n",
    "        tk_plot(mat,path_to_RAW, data_dict, fit_args, fit_range = [100e-3, np.sort(T)[-1]], points=True, fits=\"combined\", fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENAPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
