{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermal Conductivity Raw Data Fitting\n",
    "Developed by Henry Nachman\n",
    "\n",
    "Last Edited: 14 March 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json, shutil\n",
    "\n",
    "# note : most functions needed for running this notebook can be found in tc_utils.\n",
    "from tc_utils import *\n",
    "# Defines the matplotlib backend for plots\n",
    "%matplotlib qt5\n",
    "\n",
    "plots = False # Set to true to reproduce all plots, note this will likely lengthen the time to run the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find where all our RAW data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_lib = f\"{os.getcwd()}\\\\lib\"\n",
    "mat_directories = [folder for folder in os.listdir(path_to_lib) if not folder.endswith(\".md\")]\n",
    "\n",
    "path_to_RAW = dict()\n",
    "\n",
    "for mat in mat_directories:\n",
    "    path_to_mat = f\"{path_to_lib}\\\\{mat}\"\n",
    "    raw_str = f\"{path_to_mat}\\\\RAW\"\n",
    "    config_str = f\"{path_to_mat}\\\\config.yaml\"\n",
    "    other_str = f\"{path_to_mat}\\\\OTHERFITS\"\n",
    "    nist_str = f\"{path_to_mat}\\\\NIST\"\n",
    "    source = []\n",
    "    if os.path.exists(raw_str): # Finds the raw data if it exists.\n",
    "        path_to_RAW[mat] = raw_str\n",
    "        source.append(\"RAW\")\n",
    "    if os.path.exists(other_str): # Finds other fits\n",
    "        source.append(\"other\")\n",
    "    if os.path.exists(nist_str): # Finds NIST fit\n",
    "        source.append(\"NIST\")\n",
    "\n",
    "    if not os.path.exists(config_str): # Check for existing JSON\n",
    "        yaml_dict = []\n",
    "        for i in range(len(source)):\n",
    "            yaml_dict.append({\"name\":f\"{mat}\", \"parent\":\"NA\", \"source\":f\"{source[i]}\"}) # Define JSON dictionary\n",
    "        yaml_dict = json.dumps(yaml_dict, indent=4)\n",
    "        with open(config_str, 'w') as file:\n",
    "            file.write(yaml_dict) # Write to new JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aluminum1 has parent: Aluminum\n",
      "Found 1 measurements.\n",
      "Clearwater has parent: CFRP\n",
      "Found 5 measurements.\n",
      "DPP has parent: CFRP\n",
      "Found 2 measurements.\n",
      "Graphlite has parent: CFRP\n",
      "Found 2 measurements.\n",
      "SS304 has parent: Steel\n",
      "Found 8 measurements.\n",
      "SS310 has parent: Steel\n",
      "Found 3 measurements.\n",
      "SS316 has parent: Steel\n",
      "Found 6 measurements.\n",
      "SS321 has parent: Steel\n",
      "Found 6 measurements.\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON\n",
    "for mat in mat_directories:\n",
    "    path_to_mat = f\"{path_to_lib}\\\\{mat}\"\n",
    "    raw_str = f\"{path_to_mat}\\\\RAW\"\n",
    "    config_str = f\"{path_to_mat}\\\\config.yaml\"\n",
    "    # other_str = f\"{path_to_mat}\\\\OTHERFITS\"\n",
    "    with open(config_str, 'r') as file:\n",
    "        mat_config = json.load(file)\n",
    "    parent = mat_config[0][\"parent\"]\n",
    "    if parent != \"NA\":\n",
    "        print(mat, \"has parent:\", parent)\n",
    "        parent_dir = f\"{path_to_lib}\\\\{parent}\"\n",
    "        if not os.path.exists(parent_dir):\n",
    "            os.mkdir(parent_dir)\n",
    "            os.mkdir(f\"{parent_dir}\\\\RAW\")\n",
    "        raw_files = get_datafiles(raw_str)\n",
    "        for file in raw_files:\n",
    "            # print(file)\n",
    "            # try:\n",
    "            shutil.copy(f\"{raw_str}\\\\{file}\", f\"{parent_dir}\\\\RAW\\\\{file}\")\n",
    "            # except shutil.SameFileError:\n",
    "                # print(\"file already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our fitting code for every material found in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_RAW = dict()\n",
    "path_to_fits = dict()\n",
    "path_to_plots = dict()\n",
    "\n",
    "for mat in mat_directories:\n",
    "    path_to_mat = f\"{path_to_lib}\\\\{mat}\"\n",
    "    raw_str = f\"{path_to_mat}\\\\RAW\"\n",
    "    fits_str = f\"{path_to_mat}\\\\fits\"\n",
    "    plots_str = f\"{path_to_mat}\\\\plots\"\n",
    "    if os.path.exists(raw_str):\n",
    "        path_to_RAW[mat] = raw_str\n",
    "        if not os.path.exists(fits_str):\n",
    "            os.mkdir(fits_str)\n",
    "        path_to_fits[mat] = fits_str\n",
    "        if not os.path.exists(plots_str):\n",
    "            os.mkdir(plots_str)\n",
    "        path_to_fits[mat] = fits_str\n",
    "        path_to_plots[mat] = plots_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gaps(data_array, threshold):\n",
    "    # Calculate differences between consecutive elements\n",
    "    diffs = np.diff(data_array)\n",
    "\n",
    "    # Find indices where differences exceed threshold\n",
    "    major_gap_indices = np.where(diffs > threshold)[0]\n",
    "\n",
    "    # Return indices of major gaps\n",
    "    return major_gap_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aluminum : Using a low fit - 1.061 is below 20K.\n",
      "Aluminum1 : Using a low fit - 1.061 is below 20K.\n",
      "CFRP : Using a low fit - 4.842 is below 20K.\n",
      "Clearwater : Using a low fit - 4.842 is below 20K.\n",
      "DPP : Using a low fit - 4.02 is below 20K.\n",
      "Fiberglass : Using a low fit - 2.970783932 is below 20K.\n",
      "Graphlite : Using a low fit - 4.015 is below 20K.\n",
      "Ketron : Using a low fit - 2.851 is below 20K.\n",
      "Macor : Using a low fit - 3.21338073 is below 20K.\n",
      "SS304 : Using a combined fit - data exists on both sides of 20K.\n",
      "[123 125 126 129 130 131 132 133 134 135 136]\n",
      "Low-Hi split centered at : 607.1428571428571 ~~ with average percent difference value of: 14.48%\n",
      "SS310 : Using a combined fit - data exists on both sides of 20K.\n",
      "[10 58 61 63]\n",
      "Low-Hi split centered at : 12.132857142857144 ~~ with average percent difference value of: 0.94%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (67,) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow-Hi split centered at : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbesterf_loc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ~~ with average percent difference value of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbestRes\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# We will repeat the above fit with this new 'optimized' split location\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     fit_args \u001b[38;5;241m=\u001b[39m \u001b[43mdual_tc_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbig_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_plots\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merf_loc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbesterf_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_orders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m output_array \u001b[38;5;241m=\u001b[39m format_combofit(fit_args)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Finally, we will output the fit parameters as a csv, and lh5 file - and plot the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henac\\OneDrive - The University of Texas at Austin\\01_RESEARCH\\05_CMBS4\\Cryogenic_Material_Properties\\thermal_conductivity\\tc_utils.py:553\u001b[0m, in \u001b[0;36mdual_tc_fit\u001b[1;34m(big_data, save_path, erf_loc, fit_orders, fit_types, plots)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdual_tc_fit\u001b[39m(big_data, save_path, erf_loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, fit_orders \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), fit_types\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk/T\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloglog\u001b[39m\u001b[38;5;124m\"\u001b[39m), plots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    Arguments :\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m    - big_data   - Array of measurement data concatenated (should be of shape: [N, 3])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;124;03m    - arg_dict - Dictionary of fit arguments - includes low fit, high fit, and combined fit arguments.\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     dsplit \u001b[38;5;241m=\u001b[39m \u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbig_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merf_loc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m     lowT, lowT_k, lowT_koT, low_ws, hiT, hiT_k, hiT_koT, hi_ws \u001b[38;5;241m=\u001b[39m dsplit\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;66;03m# Take a log10 of the high range\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henac\\OneDrive - The University of Texas at Austin\\01_RESEARCH\\05_CMBS4\\Cryogenic_Material_Properties\\thermal_conductivity\\tc_utils.py:211\u001b[0m, in \u001b[0;36msplit_data\u001b[1;34m(big_data, erf_loc)\u001b[0m\n\u001b[0;32m    197\u001b[0m T, k, koT, weights \u001b[38;5;241m=\u001b[39m [big_data[:,\u001b[38;5;241m0\u001b[39m], big_data[:,\u001b[38;5;241m1\u001b[39m], big_data[:,\u001b[38;5;241m2\u001b[39m], big_data[:,\u001b[38;5;241m3\u001b[39m]]\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Find the low range\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# lowT, hiT = [T[T<erf_loc], T[T>erf_loc]]\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m \n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m#     erf_loc = np.mean(T)\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m low_ws, hi_ws \u001b[38;5;241m=\u001b[39m [weights[\u001b[43mT\u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43merf_loc\u001b[49m], weights[T\u001b[38;5;241m>\u001b[39merf_loc]]\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Find the low range\u001b[39;00m\n\u001b[0;32m    214\u001b[0m lowT, lowT_k, lowT_koT \u001b[38;5;241m=\u001b[39m [T[T\u001b[38;5;241m<\u001b[39merf_loc], k[T\u001b[38;5;241m<\u001b[39merf_loc], koT[T\u001b[38;5;241m<\u001b[39merf_loc]]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (67,) (5,) "
     ]
    }
   ],
   "source": [
    "for mat in path_to_RAW.keys(): # [\"SS304\"]: # \n",
    "    perc_diff_avg = np.array([])\n",
    "    ## First, let's collect the raw data from their csv files\n",
    "    big_data, data_dict = parse_raw(mat, path_to_RAW[mat], plots=False, weight_const=0.00)\n",
    "    T, k, koT, weights = [big_data[:,0], big_data[:,1], big_data[:,2], big_data[:,3]]\n",
    "\n",
    "    maxT, minT = [max(T), min(T)]\n",
    "    fit_orders = [3,3]\n",
    "    fit_types = [\"k/T\", \"loglog\"]\n",
    "\n",
    "    if (maxT <= 20):\n",
    "        print(f\"{mat} : Using a low fit - {maxT} is below 20K.\")\n",
    "        low_fit_xs, low_fit = koT_function(T, koT, fit_orders[0], weights)\n",
    "        hi_fit, hi_fit_xs, erf_loc = [[0], [0], [0]]\n",
    "        fit_args = dict_combofit(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "    elif (minT >= 20):\n",
    "        print(f\"{mat} : Using a hi fit - {minT} is above 20K.\")\n",
    "        hi_fit_xs, hi_fit = logk_function(np.log10(T), np.log10(k), fit_orders[1], weights)\n",
    "        low_fit, low_fit_xs, erf_loc = [[0], [0], [-1]]\n",
    "        fit_args = dict_combofit(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "    else:\n",
    "        print(f\"{mat} : Using a combined fit - data exists on both sides of 20K.\")\n",
    "        indices = find_gaps(np.sort(T), 50)\n",
    "        print(indices)\n",
    "        erf_locList = np.linspace(np.sort(T)[indices[0]], np.sort(T)[indices[0]+1], 15) # np.linspace(np.sort(T)[0], np.sort(T)[-1], 15) # [30] # \n",
    "        for erf_loc in erf_locList:\n",
    "            dsplit = split_data(big_data, erf_loc)\n",
    "            lowT, lowT_k, lowT_koT, low_ws, hiT, hiT_k, hiT_koT, hi_ws = dsplit\n",
    "            # Take a log10 of the high range\n",
    "            log_hi_T = np.log10(hiT)\n",
    "            log_hi_k = np.log10(hiT_k)\n",
    "            \n",
    "            if (len(lowT)==0):\n",
    "                low_fit = [0]\n",
    "                # print(f\"Only using high fit minT: {min(T)} > 20\")\n",
    "            else:\n",
    "                low_fit_xs, low_fit = koT_function(lowT, lowT_koT, fit_orders[0], low_ws)\n",
    "            if (len(hiT)==0):\n",
    "                hi_fit = [0]\n",
    "                # print(f\"Only using low fit {max(T)} < 20\")\n",
    "            else:\n",
    "                hi_fit_xs, hi_fit = logk_function(log_hi_T, log_hi_k, fit_orders[1], hi_ws)\n",
    "            fit_args = dict_combofit(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "            ## With the fit complete, let's output a formatted dictionary with the fit parameters\n",
    "            output_array = format_combofit(fit_args)\n",
    "            ## We want to figure out the best location for the split in data, so we will compute the residual of the combined fit\n",
    "            low_param, hi_param, erf_param = fit_args[\"low_fit_param\"], fit_args[\"hi_fit_param\"], fit_args[\"combined_fit_param\"][-1]\n",
    "            kpred = loglog_func(T, low_param, hi_param, erf_param)\n",
    "            # and append it to the array resVal\n",
    "            diff = abs(kpred-k)\n",
    "            perc_diff_arr = 100*diff/kpred\n",
    "            perc_diff_avg = np.append(perc_diff_avg, np.mean(perc_diff_arr))\n",
    "\n",
    "        # Now that we have found the residuals of the fits for many different split locations, let's choose the best one.    \n",
    "        erf_locdict = dict(zip(erf_locList, perc_diff_avg))\n",
    "        bestRes = min(erf_locdict.values())\n",
    "        besterf_loc = [key for key in erf_locdict if erf_locdict[key] == bestRes]\n",
    "        print(f\"Low-Hi split centered at : {besterf_loc[0]} ~~ with average percent difference value of: {bestRes:.2f}%\")\n",
    "        \n",
    "        # We will repeat the above fit with this new 'optimized' split location\n",
    "        fit_args = dual_tc_fit(big_data, path_to_plots[mat], erf_loc=besterf_loc, fit_orders=(3, 3), plots=False)\n",
    "    \n",
    "    output_array = format_combofit(fit_args)\n",
    "\n",
    "    # Finally, we will output the fit parameters as a csv, and lh5 file - and plot the data.\n",
    "    create_data_table(output_array, f\"{path_to_fits[mat]}\\\\{mat}.txt\")\n",
    "    create_tc_csv(output_array, f\"{path_to_fits[mat]}\\\\{mat}.csv\")\n",
    "    make_fit_lh5(fit_args, path_to_fits[mat])\n",
    "    # PLOTTING CODE\n",
    "    if plots:\n",
    "        tk_plot(mat,path_to_RAW, data_dict, fit_args, fit_range = [100e-3, np.sort(T)[-1]], points=True, fits=\"combined\", fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENAPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
