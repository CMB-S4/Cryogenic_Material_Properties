{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermal Conductivity Raw Data Fitting\n",
    "Developed by Henry Nachman\n",
    "\n",
    "Last Edited: 14 March 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json, shutil\n",
    "\n",
    "# note : most functions needed for running this notebook can be found in tc_utils.\n",
    "from tc_utils import *\n",
    "# Defines the matplotlib backend for plots\n",
    "%matplotlib qt5\n",
    "\n",
    "plots = True # Set to true to reproduce all plots, note this will likely lengthen the time to run the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find where all our RAW data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_lib = f\"{os.getcwd()}\\\\lib\"\n",
    "mat_directories = [folder for folder in os.listdir(path_to_lib) if not folder.endswith(\".md\")]\n",
    "\n",
    "path_to_RAW = dict()\n",
    "\n",
    "for mat in mat_directories:\n",
    "    path_to_mat = f\"{path_to_lib}\\\\{mat}\"\n",
    "    raw_str = f\"{path_to_mat}\\\\RAW\"\n",
    "    config_str = f\"{path_to_mat}\\\\config.yaml\"\n",
    "    other_str = f\"{path_to_mat}\\\\OTHERFITS\"\n",
    "    nist_str = f\"{path_to_mat}\\\\NIST\"\n",
    "    source = []\n",
    "    if os.path.exists(raw_str): # Finds the raw data if it exists.\n",
    "        path_to_RAW[mat] = raw_str\n",
    "        source.append(\"RAW\")\n",
    "    if os.path.exists(other_str): # Finds other fits\n",
    "        source.append(\"other\")\n",
    "    if os.path.exists(nist_str): # Finds NIST fit\n",
    "        source.append(\"NIST\")\n",
    "\n",
    "    # if not os.path.exists(config_str): # Check for existing JSON\n",
    "    yaml_dict = []\n",
    "    for i in range(len(source)):\n",
    "        yaml_dict.append({\"name\":f\"{mat}\", \"parent\":\"NA\", \"source\":f\"{source[i]}\"}) # Define JSON dictionary\n",
    "    yaml_dict = json.dumps(yaml_dict, indent=4)\n",
    "    with open(config_str, 'w') as file:\n",
    "        file.write(yaml_dict) # Write to new JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON\n",
    "for mat in mat_directories:\n",
    "    # path_to_mat = f\"{path_to_lib}\\\\{mat}\"\n",
    "    raw_str = f\"{path_to_mat}\\\\RAW\"\n",
    "    config_str = f\"{path_to_mat}\\\\config.yaml\"\n",
    "    # other_str = f\"{path_to_mat}\\\\OTHERFITS\"\n",
    "    with open(config_str, 'r') as file:\n",
    "        mat_config = json.load(file)\n",
    "    parent = mat_config[0][\"parent\"]\n",
    "\n",
    "    if parent != \"NA\":\n",
    "        print(mat, \"has parent:\", parent)\n",
    "        parent_dir = f\"{path_to_lib}\\\\{parent}\"\n",
    "        if not os.path.exists(parent_dir):\n",
    "            os.mkdir(parent_dir)\n",
    "            os.mkdir(f\"{parent_dir}\\\\RAW\")\n",
    "        raw_files = get_datafiles(raw_str)\n",
    "        for file in raw_files:\n",
    "            # print(file)\n",
    "            shutil.copy(f\"{raw_str}\\\\{file}\", f\"{parent_dir}\\\\RAW\\\\{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our fitting code for every material found in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_RAW = dict()\n",
    "path_to_fits = dict()\n",
    "path_to_plots = dict()\n",
    "\n",
    "for mat in mat_directories:\n",
    "    path_to_mat = f\"{path_to_lib}\\\\{mat}\"\n",
    "    raw_str = f\"{path_to_mat}\\\\RAW\"\n",
    "    fits_str = f\"{path_to_mat}\\\\fits\"\n",
    "    plots_str = f\"{path_to_mat}\\\\plots\"\n",
    "    if os.path.exists(raw_str):\n",
    "        path_to_RAW[mat] = raw_str\n",
    "        if not os.path.exists(fits_str):\n",
    "            os.mkdir(fits_str)\n",
    "        path_to_fits[mat] = fits_str\n",
    "        if not os.path.exists(plots_str):\n",
    "            os.mkdir(plots_str)\n",
    "        path_to_fits[mat] = fits_str\n",
    "        path_to_plots[mat] = plots_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aluminum : Using a low fit - 1.061 is below 20K.\n",
      "Clearwater : Using a low fit - 4.842 is below 20K.\n",
      "DPP : Using a low fit - 4.02 is below 20K.\n",
      "Fiberglass : Using a low fit - 2.970783932 is below 20K.\n",
      "Graphlite : Using a low fit - 4.015 is below 20K.\n",
      "Ketron : Using a low fit - 2.851 is below 20K.\n",
      "Macor : Using a low fit - 3.21338073 is below 20K.\n",
      "SS304 : Using a combined fit - data exists on both sides of 20K.\n",
      "Only using high fit minT: 0.3846 > 20\n",
      "Only using low fit 1672.0 < 20\n",
      "Low-Hi split centered at : 119.7857 ~~ with average percent difference value of: 3.95%\n",
      "Torlon : Using a low fit - 2.977 is below 20K.\n",
      "VESPEL : Using a low fit - 3.032 is below 20K.\n"
     ]
    }
   ],
   "source": [
    "for mat in path_to_RAW.keys(): # [\"SS304\"]: # \n",
    "    perc_diff_avg = np.array([])\n",
    "    ## First, let's collect the raw data from their csv files\n",
    "    big_data, data_dict = parse_raw(mat, path_to_RAW[mat], plots=False, weight_const=0.00)\n",
    "    T, k, koT, weights = [big_data[:,0], big_data[:,1], big_data[:,2], big_data[:,3]]\n",
    "\n",
    "    maxT, minT = [max(T), min(T)]\n",
    "    fit_orders = [3,3]\n",
    "    fit_types = [\"k/T\", \"loglog\"]\n",
    "\n",
    "    if (maxT <= 20):\n",
    "        print(f\"{mat} : Using a low fit - {maxT} is below 20K.\")\n",
    "        low_fit_xs, low_fit = koT_function(T, koT, fit_orders[0], weights)\n",
    "        hi_fit, hi_fit_xs, erf_loc = [[0], [0], [0]]\n",
    "        fit_args = dict_combofit(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "    elif (minT >= 20):\n",
    "        print(f\"{mat} : Using a hi fit - {minT} is above 20K.\")\n",
    "        hi_fit_xs, hi_fit = logk_function(np.log10(T), np.log10(k), fit_orders[1], weights)\n",
    "        low_fit, low_fit_xs, erf_loc = [[0], [0], [-1]]\n",
    "        fit_args = dict_combofit(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "    else:\n",
    "        print(f\"{mat} : Using a combined fit - data exists on both sides of 20K.\")\n",
    "        erf_locList = np.linspace(np.sort(T)[0], np.sort(T)[-1], 15) # [30] # \n",
    "        for erf_loc in erf_locList:\n",
    "            dsplit = split_data(big_data, erf_loc)\n",
    "            lowT, lowT_k, lowT_koT, low_ws, hiT, hiT_k, hiT_koT, hi_ws = dsplit\n",
    "            # Take a log10 of the high range\n",
    "            log_hi_T = np.log10(hiT)\n",
    "            log_hi_k = np.log10(hiT_k)\n",
    "            \n",
    "            if (len(lowT)==0):\n",
    "                low_fit = [0]\n",
    "                print(f\"Only using high fit minT: {min(T)} > 20\")\n",
    "            else:\n",
    "                low_fit_xs, low_fit = koT_function(lowT, lowT_koT, fit_orders[0], low_ws)\n",
    "            if (len(hiT)==0):\n",
    "                hi_fit = [0]\n",
    "                print(f\"Only using low fit {max(T)} < 20\")\n",
    "            else:\n",
    "                hi_fit_xs, hi_fit = logk_function(log_hi_T, log_hi_k, fit_orders[1], hi_ws)\n",
    "            fit_args = dict_combofit(low_fit, low_fit_xs, hi_fit, hi_fit_xs, fit_orders, fit_types, erf_loc)\n",
    "            ## With the fit complete, let's output a formatted dictionary with the fit parameters\n",
    "            output_array = format_combofit(fit_args)\n",
    "            ## We want to figure out the best location for the split in data, so we will compute the residual of the combined fit\n",
    "            low_param, hi_param, erf_param = fit_args[\"low_fit_param\"], fit_args[\"hi_fit_param\"], fit_args[\"combined_fit_param\"][-1]\n",
    "            kpred = loglog_func(T, low_param, hi_param, erf_param)\n",
    "            # and append it to the array resVal\n",
    "            diff = abs(kpred-k)\n",
    "            perc_diff_arr = 100*diff/kpred\n",
    "            perc_diff_avg = np.append(perc_diff_avg, np.mean(perc_diff_arr))\n",
    "\n",
    "        # Now that we have found the residuals of the fits for many different split locations, let's choose the best one.    \n",
    "        erf_locdict = dict(zip(erf_locList, perc_diff_avg))\n",
    "        bestRes = min(erf_locdict.values())\n",
    "        besterf_loc = [key for key in erf_locdict if erf_locdict[key] == bestRes]\n",
    "        print(f\"Low-Hi split centered at : {besterf_loc[0]} ~~ with average percent difference value of: {bestRes:.2f}%\")\n",
    "        \n",
    "        # We will repeat the above fit with this new 'optimized' split location\n",
    "        fit_args = dual_tc_fit(big_data, path_to_plots[mat], erf_loc=besterf_loc, fit_orders=(3, 3), plots=False)\n",
    "    \n",
    "    output_array = format_combofit(fit_args)\n",
    "\n",
    "    # Finally, we will output the fit parameters as a csv, and lh5 file - and plot the data.\n",
    "    create_data_table(output_array, f\"{path_to_fits[mat]}\\\\{mat}.txt\")\n",
    "    create_tc_csv(output_array, f\"{path_to_fits[mat]}\\\\{mat}.csv\")\n",
    "    make_fit_lh5(fit_args, path_to_fits[mat])\n",
    "    # PLOTTING CODE\n",
    "    if plots:\n",
    "        tk_plot(mat,path_to_RAW, data_dict, fit_args, fit_range = [100e-3, np.sort(T)[-1]], points=True, fits=\"combined\", fill=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENAPenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
